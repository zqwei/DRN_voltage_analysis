{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix Spike Dectection in Social Water condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sys import platform\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.signal import medfilt\n",
    "sns.set_style('ticks')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_file = Path('../Voltron_data/Voltron_Log_DRN_Exp.csv')\n",
    "dat_xls_file = pd.read_csv(vol_file, index_col=0)\n",
    "dat_xls_file['folder'] = dat_xls_file['folder'].apply(lambda x: f'{x:0>8}')\n",
    "# using Path to handle switches filesystems\n",
    "if platform == \"linux\" or platform == \"linux2\":\n",
    "    dir_folder = Path('/nrs/ahrens/Ziqiang/Takashi_DRN_project/ProcessedData/')\n",
    "elif platform == 'win32':\n",
    "    dir_folder = Path('U:\\\\Takashi') # put folder for windows system\n",
    "include_inds = np.array([4, 5, 11, 12, 13, 14, 22, 31, 32, 33, 34, 38, 39, 43, 52, 54])\n",
    "sigma=20;\n",
    "short_kernel=(1/(np.sqrt(2*np.pi)*sigma))*np.exp(-(np.arange(-60,61)**2)/(2*sigma**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "from trefide.temporal import TrendFilter\n",
    "\n",
    "dat_folder = '/nrs/ahrens/Ziqiang/Takashi_DRN_project/ProcessedData/'\n",
    "\n",
    "def single_x(voltr, window_length=41, win_=50001):\n",
    "    from fish_proc.spikeDetectionNN.utils import roll_scale\n",
    "    from fish_proc.spikeDetectionNN.spikeDetector import prepare_sequences_center\n",
    "    if voltr.ndim>1:\n",
    "        voltr = voltr.reshape(-1)\n",
    "    voltr_ = voltr[600:]\n",
    "    n_spk = np.zeros(len(voltr_)).astype('bool')\n",
    "    voltr_ = roll_scale(voltr_, win_=win_)\n",
    "    x_, _ = prepare_sequences_center(voltr_, n_spk, window_length, peak_wid=2)\n",
    "    return voltr_[np.newaxis, :], np.expand_dims(x_, axis=0)\n",
    "\n",
    "\n",
    "def voltr2spike_(voltrs, window_length, m):\n",
    "    from fish_proc.utils.np_mp import parallel_to_single\n",
    "    from fish_proc.spikeDetectionNN.utils import detected_window_max_spike\n",
    "    from fish_proc.spikeDetectionNN.utils import cluster_spikes\n",
    "    import time\n",
    "    start = time.time()\n",
    "    voltr_list, x_list = parallel_to_single(single_x, voltrs, window_length=window_length)\n",
    "    print(time.time() - start)\n",
    "    n_, len_ = voltr_list.shape\n",
    "    spk1_list = np.empty(voltr_list.shape)\n",
    "    spk2_list = np.empty(voltr_list.shape)\n",
    "    spk_list = np.empty(voltr_list.shape)\n",
    "    spkprob_list = np.empty(voltr_list.shape)\n",
    "    for _, (voltr_, x_) in enumerate(zip(voltr_list, x_list)):\n",
    "        start = time.time()\n",
    "        pred_x_test = m.predict(x_)\n",
    "        spk_, spkprob = detected_window_max_spike(pred_x_test, voltr_, window_length = window_length, peak_wid=2, thres=0.5)\n",
    "        spk1, spk2 = cluster_spikes(spk_, spkprob, voltr_)\n",
    "        spk_list[_, :] = spk_\n",
    "        spk1_list[_, :] = spk1\n",
    "        spk2_list[_, :] = spk2\n",
    "        spkprob_list[_, :] = spkprob\n",
    "        print(f'Spike detection for neuron #{_} is done......')\n",
    "        print(time.time() - start)\n",
    "    print('Spike detection done for all neurons')\n",
    "    return spk_list, spkprob_list, spk1_list, spk2_list, voltr_list\n",
    "\n",
    "\n",
    "def tf_filter(_):\n",
    "    from trefide.temporal import TrendFilter\n",
    "    spk__, voltr_, voltr= _\n",
    "    filters = TrendFilter(len(voltr_))\n",
    "    tspk = np.where(spk__>0)[0]\n",
    "    tspk_win = tspk[:, None] + np.arange(-3, 3)[None, :]\n",
    "    tspk_win = tspk_win.reshape(-1)\n",
    "    nospike = np.zeros(spk__.shape)\n",
    "    nospike[tspk_win] = 1\n",
    "    tspk_ = np.where(nospike==0)[0]\n",
    "    int_voltr_ = voltr_.copy()\n",
    "    int_voltr_[tspk_win] = np.interp(tspk_win, tspk_, voltr_[tspk_])\n",
    "    denoised_voltr_ = filters.denoise(int_voltr_)\n",
    "\n",
    "    int_voltr_ = voltr[600:].copy()\n",
    "    int_voltr_[tspk_win] = np.interp(tspk_win, tspk_, voltr[600:][tspk_])\n",
    "    denoised_voltr = filters.denoise(int_voltr_)\n",
    "    out = (denoised_voltr_, denoised_voltr)\n",
    "    return np.asarray(out[0])[np.newaxis,:], np.asarray(out[1])[np.newaxis,:]\n",
    "\n",
    "\n",
    "def plot_components(A_, Y_trend_ave, fext='', save_folder='', save_image_folder=''):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    sns.set(font_scale=2)\n",
    "    sns.set_style(\"white\")\n",
    "\n",
    "    d1, d2 = Y_trend_ave.shape\n",
    "    A_comp = np.zeros(A_.shape[0])\n",
    "    A_comp[A_.sum(axis=-1)>0] = np.argmax(A_[A_.sum(axis=-1)>0, :], axis=-1) + 1\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.imshow(Y_trend_ave, cmap=plt.cm.gray)\n",
    "    plt.imshow(A_comp.reshape(d2, d1).T, cmap=plt.cm.nipy_spectral_r, alpha=0.7)\n",
    "    for n, nA in enumerate(A_.T):\n",
    "        nA = nA.reshape(d2, d1).T\n",
    "        pos = np.where(nA>0);\n",
    "        pos0 = pos[0];\n",
    "        pos1 = pos[1];\n",
    "        plt.text(pos1.mean(), pos0.mean(), f\"{n}\", fontsize=15)\n",
    "    plt.title('Components')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_image_folder}/Demixed_components{fext}.png')\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.imshow(A_.sum(axis=-1).reshape(d2, d1).T)\n",
    "    for n, nA in enumerate(A_.T):\n",
    "        nA = nA.reshape(d2, d1).T\n",
    "        pos = np.where(nA>0);\n",
    "        pos0 = pos[0];\n",
    "        pos1 = pos[1];\n",
    "        plt.text(pos1.mean(), pos0.mean(), f\"{n}\", fontsize=15, color='w')\n",
    "    plt.title('Components weights')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_image_folder}/Demixed_components_weights{fext}.png')\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def voltron(row, fext='', is_mask=False):\n",
    "    from pathlib import Path\n",
    "    from skimage.external.tifffile import imread\n",
    "    from fish_proc.utils.demix import recompute_C_matrix, pos_sig_correction\n",
    "    import pickle\n",
    "\n",
    "    folder = row['folder']\n",
    "    fish = row['fish']\n",
    "    save_folder = dat_folder + f'{folder}/{fish}/Data'\n",
    "    save_image_folder = dat_folder + f'{folder}/{fish}/Results'\n",
    "\n",
    "    if not os.path.exists(save_image_folder):\n",
    "        os.makedirs(save_image_folder)\n",
    "    print('=====================================')\n",
    "    print(save_folder)\n",
    "\n",
    "    if os.path.isfile(save_folder+f'/finished_voltr{fext}.tmp'):\n",
    "        return None\n",
    "\n",
    "    if not os.path.isfile(f'{save_folder}/period_Y_demix{fext}_rlt.pkl'):\n",
    "        print('Components file does not exist.')\n",
    "        return None\n",
    "\n",
    "    if os.path.isfile(save_folder+f'/proc_voltr{fext}.tmp'):\n",
    "        print('File is already in processing.')\n",
    "        return None\n",
    "\n",
    "    Path(save_folder+f'/proc_voltr{fext}.tmp').touch()\n",
    "    Y_trend_ave = np.load(f'{save_folder}/Y_trend_ave.npy')\n",
    "    if is_mask:\n",
    "        _ = np.load(f'{save_folder}/mask.npz')\n",
    "        mask = _['mask']\n",
    "        mask_save = _['mask_save']\n",
    "        Y_trend_ave = Y_trend_ave[mask_save[0].min():mask_save[0].max(), mask_save[1].min():mask_save[1].max()]\n",
    "\n",
    "    print('update components images')\n",
    "    with open(f'{save_folder}/period_Y_demix{fext}_rlt.pkl', 'rb') as f:\n",
    "        rlt_ = pickle.load(f)\n",
    "    d1, d2 = Y_trend_ave.shape\n",
    "\n",
    "    mask_ = np.empty((d2, d1))\n",
    "    mask_[:] = False\n",
    "    if is_mask:\n",
    "        pixel = 1\n",
    "    else:\n",
    "        pixel = 4\n",
    "    mask_[:pixel, :]=True\n",
    "    mask_[-pixel:,:]=True\n",
    "    mask_[:, :pixel]=True\n",
    "    mask_[:,-pixel:]=True\n",
    "    mask_ = mask_.astype('bool')\n",
    "    A = rlt_['fin_rlt']['a'].copy()\n",
    "    # remove bounds at pixel size\n",
    "    A[mask_.reshape(-1),:]=0\n",
    "\n",
    "    # remove small size components\n",
    "    A_ = A[:, (A>0).sum(axis=0)>40] # min pixel = 40\n",
    "\n",
    "    # remove low weight pixels for all components\n",
    "    # wid_mat = A_.sum(axis=-1)\n",
    "    # thres_ = np.percentile(A_.sum(axis=-1), 90)\n",
    "    # wid_mat[wid_mat<thres_] = 0\n",
    "    # A_[wid_mat<thres_, :]=0\n",
    "\n",
    "    # remove stripes\n",
    "    remove_comp = np.empty(A_.shape[-1]).astype('bool')\n",
    "    remove_comp[:] = False\n",
    "    for n_, nA_ in enumerate(A_.T):\n",
    "        x_, y_ = np.where(nA_.reshape(d2, d1).T>0)\n",
    "        len_x = x_.max()-x_.min()\n",
    "        len_y = y_.max()-y_.min()\n",
    "        if len_x/len_y>10 or len_y/len_x>10 or len_x<=3 or len_y<=3:\n",
    "            remove_comp[n_] = True\n",
    "    A_ = A_[:, ~remove_comp]\n",
    "\n",
    "    plot_components(A_, Y_trend_ave, fext=fext, save_folder=save_folder, save_image_folder=save_image_folder)\n",
    "\n",
    "    print('Start computing voltron data')\n",
    "    _ = np.load(f'{save_folder}/Y_2dnorm.npz')\n",
    "    Y_d_std= _['Y_d_std']\n",
    "    mov = -imread(f'{save_folder}/Y_svd.tif').astype('float32')*Y_d_std\n",
    "    mov = mov[mask_save[0].min():mask_save[0].max(), mask_save[1].min():mask_save[1].max(), :]\n",
    "\n",
    "\n",
    "    b = rlt_['fin_rlt']['b']\n",
    "    fb = rlt_['fin_rlt']['fb']\n",
    "    ff = rlt_['fin_rlt']['ff']\n",
    "    dims = mov.shape\n",
    "    if fb is not None:\n",
    "        b_ = np.matmul(fb, ff.T)+b\n",
    "    else:\n",
    "        b_ = b\n",
    "    mov = pos_sig_correction(mov, -1)\n",
    "    mov = mov - b_.reshape((dims[0], dims[1], len(b_)//dims[0]//dims[1]), order='F')\n",
    "    C_ = recompute_C_matrix(mov, A_)\n",
    "    base_ = recompute_C_matrix(Y_trend_ave[:, :, np.newaxis], A_)\n",
    "    np.savez_compressed(f'{save_folder}/Voltr_raw{fext}', A_=A_, C_=C_, base_=base_)\n",
    "    Path(save_folder+f'/finished_voltr{fext}.tmp').touch()\n",
    "    return None\n",
    "\n",
    "\n",
    "def voltr2spike(row, fext=''):\n",
    "    '''\n",
    "    There seems to be a limitation of cores keras can use, 4 - 8 cores are enough for this one.\n",
    "    '''\n",
    "    import tensorflow as tf\n",
    "    from keras import backend as K\n",
    "    K.set_session(K.tf.Session(config=K.tf.ConfigProto(intra_op_parallelism_threads=32, inter_op_parallelism_threads=32)))\n",
    "    import keras\n",
    "    from keras.models import load_model\n",
    "    from fish_proc.spikeDetectionNN.spikeDetector import prepare_sequences_center\n",
    "    from fish_proc.spikeDetectionNN.utils import detected_window_max_spike\n",
    "    from fish_proc.spikeDetectionNN.utils import roll_scale\n",
    "    from fish_proc.spikeDetectionNN.utils import cluster_spikes\n",
    "    from glob import glob\n",
    "    from pathlib import Path\n",
    "    trained_model = '/groups/ahrens/home/weiz/codes_repo/fish_processing/notebooks/simEphysImagingData/partly_trained_spikeDetector_2018_09_27_01_25_36.h5'\n",
    "    m = load_model(trained_model)\n",
    "    window_length = 41\n",
    "\n",
    "    folder = row['folder']\n",
    "    fish = row['fish']\n",
    "    save_folder = dat_folder + f'{folder}/{fish}/Data'\n",
    "\n",
    "    if os.path.isfile(save_folder+f'/finished_spikes{fext}.tmp'):\n",
    "        return None\n",
    "    \n",
    "    if not os.path.isfile(save_folder+f'/finished_voltr{fext}.tmp'):\n",
    "        print('Voltr file does not exist.')\n",
    "        return None\n",
    "\n",
    "    if os.path.isfile(save_folder+f'/proc_spikes{fext}.tmp'):\n",
    "        print('SPike file is already in processing.')\n",
    "        return None\n",
    "\n",
    "    Path(save_folder+f'/proc_spikes{fext}.tmp').touch()\n",
    "    _ = np.load(f'{save_folder}/Voltr_raw{fext}.npz')\n",
    "    A_ = _['A_']\n",
    "    C_ = _['C_']\n",
    "    base_ = _['base_']\n",
    "    voltrs = C_/(C_.mean(axis=-1, keepdims=True)+base_)\n",
    "    spk_list, spkprob_list, spk1_list, spk2_list, voltr_list = voltr2spike_(voltrs, window_length, m)\n",
    "    np.savez_compressed(f'{save_folder}/Voltr_spikes', voltrs=voltrs, \\\n",
    "                        spk=spk_list, spkprob=spkprob_list, spk1=spk1_list, \\\n",
    "                        spk2=spk2_list, voltr_=voltr_list)\n",
    "    Path(save_folder+f'/finished_spikes{fext}.tmp').touch()\n",
    "    return None\n",
    "\n",
    "def voltr2subvolt(row, fext=''):\n",
    "    '''\n",
    "    This one can be benefited from multiple cores.\n",
    "    '''\n",
    "    from pathlib import Path\n",
    "    import multiprocessing as mp\n",
    "\n",
    "    folder = row['folder']\n",
    "    fish = row['fish']\n",
    "    save_folder = dat_folder + f'{folder}/{fish}/Data'\n",
    "\n",
    "    if os.path.isfile(save_folder+f'/finished_subvolt{fext}.tmp'):\n",
    "        return None\n",
    "    \n",
    "    if not os.path.isfile(save_folder+f'/finished_spikes{fext}.tmp'):\n",
    "        print('Spike file does not exist.')\n",
    "        return None\n",
    "\n",
    "    # if os.path.isfile(save_folder+f'/proc_subvolt{fext}.tmp'):\n",
    "    #     print('SubVolt file is already in processing.')\n",
    "    #     return None\n",
    "\n",
    "    Path(save_folder+f'/proc_subvolt{fext}.tmp').touch()\n",
    "    print(f'Processing {save_folder}')\n",
    "    _ = np.load(f'{save_folder}/Voltr_spikes.npz')\n",
    "    voltrs = _['voltrs']\n",
    "    spk = _['spk']\n",
    "    spkprob = _['spkprob']\n",
    "    spk1 = _['spk1']\n",
    "    spk2 = _['spk2']\n",
    "    voltr_ = _['voltr_']\n",
    "    n_, len_ = voltrs.shape\n",
    "    spk__list = spk1+spk2\n",
    "    dat_ = [(spk__list[_, :], voltr_[_, :], voltrs[_, :]) for _ in range(n_)]\n",
    "    mp_count = min(mp.cpu_count(), n_)\n",
    "    pool = mp.Pool(processes=mp_count)\n",
    "    individual_results = pool.map(tf_filter, dat_)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    results = ()\n",
    "    for i_tuple in range(len(individual_results[0])):\n",
    "        results = results + (np.concatenate([_[i_tuple] for _ in individual_results]), )\n",
    "\n",
    "    np.savez_compressed(f'{save_folder}/Voltr_subvolt', norm_subvolt=results[0], subvolt_=results[1])\n",
    "    Path(save_folder+f'/finished_subvolt{fext}.tmp').touch()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'DRN_voltr_spikes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-55176a7eda0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mDRN_voltr_spikes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# for index, row in dat_xls_file.iterrows():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'DRN_voltr_spikes'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.abspath('./'))\n",
    "\n",
    "\n",
    "# for index, row in dat_xls_file.iterrows():\n",
    "#     folder = row['folder']\n",
    "#     fish = row['fish']\n",
    "#     task_type = row['task']\n",
    "#     if task_type[0] != 'S':\n",
    "#         continue\n",
    "#     dat_dir = dir_folder/f'{folder}/{fish}/Data/'\n",
    "#     swim_dir = dir_folder/f'{folder}/{fish}/swim/'\n",
    "#     frame_swim_tcourse = np.load(swim_dir/'frame_swim_tcourse_series.npy')\n",
    "#     _ = np.load(f'{dat_dir}/Voltr_spikes.npz')\n",
    "#     voltrs = _['voltrs']\n",
    "#     voltrs = voltrs - np.median(voltrs)\n",
    "#     voltr_ = _['voltr_']\n",
    "#     spk = _['spk']\n",
    "#     num_cell = spk.shape[0]\n",
    "#     spk_ = np.r_['-1', np.zeros((num_cell, 600)), spk]\n",
    "#     # length of frame_swim_tcourse is smaller than voltrs\n",
    "#     for n_cell in range(num_cell):\n",
    "# #         plt.figure(figsize=(20, 3))\n",
    "# #         plt.plot(voltrs[n_cell, :])\n",
    "# #         plt.plot(spk_[n_cell, :]*voltrs.max(), '.')\n",
    "#         plt.plot(voltr_[n_cell, :])\n",
    "#         plt.plot(spk[n_cell, :]*voltrs.max(), '.')\n",
    "# #         plt.plot(frame_swim_tcourse[0, :]*1000)\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
